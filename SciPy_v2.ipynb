{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciPy_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMK7oM7b5n1K7xKVX86a+Z1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing packages"
      ],
      "metadata": {
        "id": "qlSnNwJu5IqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xclABXTC5DMO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset, Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as fn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "4utXXiJI5LEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'ScientificPython.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoDl__nm6N6-",
        "outputId": "39e34b23-5002-4e0d-bda9-72268bbc2c94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget: missing URL\n",
            "Usage: wget [OPTION]... [URL]...\n",
            "\n",
            "Try `wget --help' for more options.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "OyGhSjxZ9_VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     transforms.resize((128, 128)))] #resizing every image to the same size\n",
        ")"
      ],
      "metadata": {
        "id": "eTi254O_-Mhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasource = os.listdir(path = 'ScientificPython/')\n",
        "train_datasource.sort()\n",
        "train_datasource = train_datasource[:2] #we only need to first two folders as they contain the training data\n",
        "\n",
        "idx_neuron = 0 \n",
        "\n",
        "train_img = []\n",
        "train_cat = [] #category = 0 or 1 (non-neuron or neuron image) from the name of the forlder\n",
        "for filename in train_datasource:\n",
        "  current_filename = os.listdir('ScientificPython/' + filename) #we iterate through the folders 0 and 1\n",
        "  current_filename.sort()\n",
        "  for img_name in current_filename:\n",
        "    img = cv2.imread(f'ScientificPython/{filename}/{img_name}') #we read the images from the folder\n",
        "    img = preprocess(img) #applying the preprocess function\n",
        "    train_img.append(img)\n",
        "    train_cat.append(torch.tensor(idx_neuron)) #filename will be the category/label\n",
        "  idx_neuron += 1\n",
        "\n",
        "train_img = torch.stack(train_img), dim=0)\n",
        "train_cat = torch.stack(train_cat, dim = 0)\n",
        "train_img.shape, train_cat.shape "
      ],
      "metadata": {
        "id": "OgXmDwvm-CyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "dX6-2BNSB6o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution"
      ],
      "metadata": {
        "id": "nzIpYTlQB9fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Conv, self).__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.conv(x)"
      ],
      "metadata": {
        "id": "bhiyDJSqB8vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downscaling"
      ],
      "metadata": {
        "id": "xP1J-yS1B_LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module): \n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__() \n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            Conv(in_channels, out_channels)\n",
        "        )\n",
        "    def forward(self, x): \n",
        "        return self.maxpool_conv(x)"
      ],
      "metadata": {
        "id": "4OWSTJ67CAIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upscaling"
      ],
      "metadata": {
        "id": "E76-hVyMCAwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels): \n",
        "        super().__init__() \n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = Conv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "eLZgQs04CBYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#constructing the UNet with the above defined functions\n",
        "class Unet(nn.Module):\n",
        "  def __init__(self, i_ch_n, o_ch_n):\n",
        "    super(Unet, self).__init__()\n",
        "\n",
        "    # parameters:\n",
        "    self.input_channel_num = i_ch_n\n",
        "    self.output_channel_num = o_ch_n\n",
        "    #self.bilinear = bilinear\n",
        "\n",
        "    # downscaleing layers\n",
        "    self.inc = Conv(self.input_channel_num, 16)\n",
        "    self.down1 = Conv(16, 32)\n",
        "    self.down2 = Down(32, 64)\n",
        "    self.down3 = Down(64, 128)\n",
        "\n",
        "    # upscaling layers\n",
        "    self.up1 = Up(128, 64)\n",
        "    self.up2 = Up(64, 32)\n",
        "    self.up3 = Up(32, 16)\n",
        "    self.outconv = nn.Conv2d(16, self.output_channel_num, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x1 = self.inc(x)\n",
        "      #print(\"x1: \" + str(x1.size()))\n",
        "      x2 = self.down1(x1)\n",
        "      #print(\"x2: \" + str(x2.size()))\n",
        "      x3 = self.down2(x2)\n",
        "      #print(\"x3: \" + str(x3.size()))\n",
        "      x4 = self.down3(x3)\n",
        "      #print(\"x4: \" + str(x4.size()))\n",
        "\n",
        "      x5 = self.up1(x4, x3)\n",
        "      #print(\"x5: \" + str(x5.size()))\n",
        "      x6 = self.up2(x5, x2)\n",
        "      #print(\"x6: \" + str(x6.size()))\n",
        "      x7 = self.up3(x6, x1)\n",
        "      #print(\"x7: \" + str(x7.size()))\n",
        "      logits = self.outconv(x7)\n",
        "\n",
        "      return logits"
      ],
      "metadata": {
        "id": "ePH-WvsYC7ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(3,3)\n",
        "test_input = torch.randn((1,3,446,446))\n",
        "output = model(test_input)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "BBTnZf-mC_30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "id": "tAH17mdKDFTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "jizndnWIDJo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_batch(img, cat, batch_indexes):\n",
        "    assert len(img) == len(cat)\n",
        "    \n",
        "    imgs = img[batch_indexes, :]\n",
        "    cats = cat[batch_indexes, :]\n",
        "\n",
        "    imgs = np.asanyarray([cv2.resize(img, dsize=(450, 450), interpolation=cv2.INTER_CUBIC) for img in imgs])\n",
        "    cats = np.asanyarray([cv2.resize(img, dsize=(450, 450), interpolation=cv2.INTER_CUBIC) for img in cats])\n",
        "\n",
        "    imgs = np.swapaxes(imgs,1,3)\n",
        "    cats = np.swapaxes(cats,1,3)\n",
        "\n",
        "    imgs = torch.from_numpy(imgs)\n",
        "    cats = torch.from_numpy(cats)\n",
        "    return imgs, cats"
      ],
      "metadata": {
        "id": "Kqx_xlU4DKxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have 3 different parameter-sets\n",
        "epochs=2\n",
        "batch_size=8\n",
        "lr=0.001"
      ],
      "metadata": {
        "id": "xF2LfOhrKAGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2\n",
        "batch_size=4\n",
        "lr=0.0001"
      ],
      "metadata": {
        "id": "Ck0QX7jEKsOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2\n",
        "batch_size=2\n",
        "lr=0.00001"
      ],
      "metadata": {
        "id": "hqBz-sGPKu7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "L9meaCxmLPza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Unet(3,3).cuda()\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
        "\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "BnRk7kXRKytY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction vs reality\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0\n",
        "  net.train()\n",
        "\n",
        "  indexes = np.arange(0,len(train_img))\n",
        "  np.random.shuffle(indexes)\n",
        "  \n",
        "\n",
        "  for iter in range(len(train_img)//batch_size):\n",
        "\n",
        "    print(\"Iter: {0}/{1}\".format(str(iter),str(len(train_img)//batch_size)),end='\\r')\n",
        "\n",
        "    batch_indexes = indexes[iter*batch_size:iter*batch_size+batch_size]\n",
        "    imgs, true_cat = get_next_batch(train_img, train_cat, batch_indexes)\n",
        "\n",
        "    assert imgs.shape[1] == net.input_channel_num, \\\n",
        "      f'Network has been defined with {net.input_channel_num} input channels, ' \\\n",
        "      f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
        "      'the images are loaded correctly.'\n",
        "    \n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    cat_type = torch.float32 #if net.input_channel_num == 1 else torch.long\n",
        "    true_masks = true_masks.to(device=device, dtype=cat_type)\n",
        "\n",
        "    cat_pred = net(imgs)\n",
        "\n",
        "    loss = criterion(cat_pred, true_cat)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(\"loss: \" + str(epoch_loss))\n",
        "    losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "2du7VyGJK2M4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}